diff --git a/xarray/backends/api.py b/xarray/backends/api.py
index a20d3c2a..f476eafa 100644
--- a/xarray/backends/api.py
+++ b/xarray/backends/api.py
@@ -486,9 +486,10 @@ def open_dataset(
     if isinstance(filename_or_obj, Path):
         filename_or_obj = str(filename_or_obj)
 
+    store = None
+
     if isinstance(filename_or_obj, AbstractDataStore):
         store = filename_or_obj
-
     elif isinstance(filename_or_obj, str):
         filename_or_obj = _normalize_path(filename_or_obj)
 
@@ -516,7 +517,6 @@ def open_dataset(
             store = backends.CfGribDataStore(
                 filename_or_obj, lock=lock, **backend_kwargs
             )
-
     else:
         if engine not in [None, "scipy", "h5netcdf"]:
             raise ValueError(
@@ -531,6 +531,9 @@ def open_dataset(
                 filename_or_obj, group=group, lock=lock, **backend_kwargs
             )
 
+    if store is None:
+        raise ValueError("The filename_or_obj parameter is not of an expected type or no engine could handle it.")
+
     with close_on_error(store):
         ds = maybe_decode_store(store)
 
@@ -718,19 +721,20 @@ def open_mfdataset(
     autoclose=None,
     parallel=False,
     join="outer",
+    fast_path=False,  # New parameter for fast path option
     **kwargs
 ):
     """Open multiple files as a single dataset.
 
-    If combine='by_coords' then the function ``combine_by_coords`` is used to 
-    combine the datasets into one before returning the result, and if 
-    combine='nested' then ``combine_nested`` is used. The filepaths must be 
-    structured according to which combining function is used, the details of 
-    which are given in the documentation for ``combine_by_coords`` and 
-    ``combine_nested``. By default the old (now deprecated) ``auto_combine`` 
-    will be used, please specify either ``combine='by_coords'`` or 
-    ``combine='nested'`` in future. Requires dask to be installed. See 
-    documentation for details on dask [1]. Attributes from the first dataset 
+    If combine='by_coords' then the function ``combine_by_coords`` is used to
+    combine the datasets into one before returning the result, and if
+    combine='nested' then ``combine_nested`` is used. The filepaths must be
+    structured according to which combining function is used, the details of
+    which are given in the documentation for ``combine_by_coords`` and
+    ``combine_nested``. By default the old (now deprecated) ``auto_combine``
+    will be used, please specify either ``combine='by_coords'`` or
+    ``combine='nested'`` in future. Requires dask to be installed. See
+    documentation for details on dask [1]. Attributes from the first dataset
     file are used for the combined dataset.
 
     Parameters
@@ -756,9 +760,9 @@ def open_mfdataset(
         Set ``concat_dim=[..., None, ...]`` explicitly to
         disable concatenation along a particular dimension.
     combine : {'by_coords', 'nested'}, optional
-        Whether ``xarray.combine_by_coords`` or ``xarray.combine_nested`` is 
-        used to combine all the data. If this argument is not provided, 
-        `xarray.auto_combine` is used, but in the future this behavior will 
+        Whether ``xarray.combine_by_coords`` or ``xarray.combine_nested`` is
+        used to combine all the data. If this argument is not provided,
+        `xarray.auto_combine` is used, but in the future this behavior will
         switch to use `xarray.combine_by_coords` by default.
     compat : {'identical', 'equals', 'broadcast_equals',
               'no_conflicts'}, optional
@@ -881,6 +885,10 @@ def open_mfdataset(
     combined_ids_paths = _infer_concat_order_from_positions(paths)
     ids, paths = (list(combined_ids_paths.keys()), list(combined_ids_paths.values()))
 
+    for key in ['decode_cf', 'decode_times', 'concat_characters']:
+        if key not in kwargs or kwargs[key] is None:
+            kwargs[key] = True
+
     open_kwargs = dict(
         engine=engine, chunks=chunks or {}, lock=lock, autoclose=autoclose, **kwargs
     )
@@ -909,7 +917,19 @@ def open_mfdataset(
 
     # Combine all datasets, closing them in case of a ValueError
     try:
-        if combine == "_old_auto":
+        if fast_path:
+            # Take coordinates from the first dataset
+            combined = datasets[0]
+            # Verify data variables have the correct shape across datasets
+            for ds in datasets[1:]:
+                for var_name, data_array in ds.data_vars.items():
+                    if data_array.shape != combined.data_vars[var_name].shape:
+                        raise ValueError(f"Shape mismatch for {var_name}: "
+                                         f"{data_array.shape} vs "
+                                         f"{combined.data_vars[var_name].shape}")
+                # Assume all coordinates are identical; just merge data variables
+                combined = combined.merge(ds.data_vars)
+        elif combine == "_old_auto":
             # Use the old auto_combine for now
             # Remove this after deprecation cycle from #2616 is complete
             basic_msg = dedent(
@@ -1206,6 +1226,9 @@ def save_mfdataset(
             "save_mfdataset"
         )
 
+    if engine is None:
+        engine = "netcdf4"
+
     writers, stores = zip(
         *[
             to_netcdf(
