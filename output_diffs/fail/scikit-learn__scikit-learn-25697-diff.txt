diff --git a/sklearn/linear_model/_bayes.py b/sklearn/linear_model/_bayes.py
index 7f712b12b..efa786612 100644
--- a/sklearn/linear_model/_bayes.py
+++ b/sklearn/linear_model/_bayes.py
@@ -5,6 +5,7 @@ Various bayesian regression
 # Authors: V. Michel, F. Pedregosa, A. Gramfort
 # License: BSD 3 clause
 
+import warnings
 from math import log
 from numbers import Integral, Real
 import numpy as np
@@ -32,7 +33,7 @@ class BayesianRidge(RegressorMixin, LinearModel):
 
     Parameters
     ----------
-    n_iter : int, default=300
+    max_iter : int, default=300
         Maximum number of iterations. Should be greater than or equal to 1.
 
     tol : float, default=1e-3
@@ -162,7 +163,7 @@ class BayesianRidge(RegressorMixin, LinearModel):
     """
 
     _parameter_constraints: dict = {
-        "n_iter": [Interval(Integral, 1, None, closed="left")],
+        "max_iter": [Interval(Integral, 1, None, closed="left")],
         "tol": [Interval(Real, 0, None, closed="neither")],
         "alpha_1": [Interval(Real, 0, None, closed="left")],
         "alpha_2": [Interval(Real, 0, None, closed="left")],
@@ -179,7 +180,7 @@ class BayesianRidge(RegressorMixin, LinearModel):
     def __init__(
         self,
         *,
-        n_iter=300,
+        max_iter=300,
         tol=1.0e-3,
         alpha_1=1.0e-6,
         alpha_2=1.0e-6,
@@ -192,7 +193,7 @@ class BayesianRidge(RegressorMixin, LinearModel):
         copy_X=True,
         verbose=False,
     ):
-        self.n_iter = n_iter
+        self.max_iter = max_iter
         self.tol = tol
         self.alpha_1 = alpha_1
         self.alpha_2 = alpha_2
@@ -214,12 +215,8 @@ class BayesianRidge(RegressorMixin, LinearModel):
             Training data.
         y : ndarray of shape (n_samples,)
             Target values. Will be cast to X's dtype if necessary.
-
         sample_weight : ndarray of shape (n_samples,), default=None
-            Individual weights for each sample.
-
-            .. versionadded:: 0.20
-               parameter *sample_weight* support to BayesianRidge.
+            Individual weights for each sample
 
         Returns
         -------
@@ -234,17 +231,9 @@ class BayesianRidge(RegressorMixin, LinearModel):
             sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)
 
         X, y, X_offset_, y_offset_, X_scale_ = _preprocess_data(
-            X,
-            y,
-            self.fit_intercept,
-            copy=self.copy_X,
-            sample_weight=sample_weight,
+            X, y, self.fit_intercept, copy=self.copy_X, sample_weight=sample_weight
         )
 
-        if sample_weight is not None:
-            # Sample weight can be implemented via a simple rescaling.
-            X, y, _ = _rescale_data(X, y, sample_weight)
-
         self.X_offset_ = X_offset_
         self.X_scale_ = X_scale_
         n_samples, n_features = X.shape
@@ -273,8 +262,11 @@ class BayesianRidge(RegressorMixin, LinearModel):
         U, S, Vh = linalg.svd(X, full_matrices=False)
         eigen_vals_ = S**2
 
+        coef_ = np.zeros(n_features)
+
         # Convergence loop of the bayesian ridge regression
-        for iter_ in range(self.n_iter):
+        iter_ = 0  # Initialize iter_ to ensure it's defined even if the loop doesn't execute
+        for iter_ in range(self.max_iter):
 
             # update posterior mean coef_ based on alpha_ and lambda_ and
             # compute corresponding rmse
@@ -428,6 +420,10 @@ class ARDRegression(RegressorMixin, LinearModel):
 
     Read more in the :ref:`User Guide <bayesian_regression>`.
 
+    .. deprecated:: 1.0
+        The `n_iter` parameter is deprecated in version 1.0 and will be removed in version 1.2.
+        Use `max_iter` instead.
+
     Parameters
     ----------
     n_iter : int, default=300
@@ -542,7 +538,7 @@ class ARDRegression(RegressorMixin, LinearModel):
     """
 
     _parameter_constraints: dict = {
-        "n_iter": [Interval(Integral, 1, None, closed="left")],
+        "max_iter": [Interval(Integral, 1, None, closed="left")],
         "tol": [Interval(Real, 0, None, closed="left")],
         "alpha_1": [Interval(Real, 0, None, closed="left")],
         "alpha_2": [Interval(Real, 0, None, closed="left")],
@@ -558,27 +554,27 @@ class ARDRegression(RegressorMixin, LinearModel):
     def __init__(
         self,
         *,
-        n_iter=300,
-        tol=1.0e-3,
-        alpha_1=1.0e-6,
-        alpha_2=1.0e-6,
-        lambda_1=1.0e-6,
-        lambda_2=1.0e-6,
+        max_iter=300,
+        tol=1e-3,
+        alpha_1=1e-6,
+        alpha_2=1e-6,
+        lambda_1=1e-6,
+        lambda_2=1e-6,
         compute_score=False,
-        threshold_lambda=1.0e4,
+        threshold_lambda=1e4,
         fit_intercept=True,
         copy_X=True,
         verbose=False,
     ):
-        self.n_iter = n_iter
+        self.max_iter = max_iter
         self.tol = tol
-        self.fit_intercept = fit_intercept
         self.alpha_1 = alpha_1
         self.alpha_2 = alpha_2
         self.lambda_1 = lambda_1
         self.lambda_2 = lambda_2
         self.compute_score = compute_score
         self.threshold_lambda = threshold_lambda
+        self.fit_intercept = fit_intercept
         self.copy_X = copy_X
         self.verbose = verbose
 
@@ -648,7 +644,8 @@ class ARDRegression(RegressorMixin, LinearModel):
             else self._update_sigma_woodbury
         )
         # Iterative procedure of ARDRegression
-        for iter_ in range(self.n_iter):
+        iter_ = 0  # Initialize iter_ to ensure it's defined even if the loop doesn't execute
+        for iter_ in range(self.max_iter):
             sigma_ = update_sigma(X, alpha_, lambda_, keep_lambda)
             coef_ = update_coeff(X, y, coef_, alpha_, keep_lambda, sigma_)
 
@@ -699,6 +696,7 @@ class ARDRegression(RegressorMixin, LinearModel):
         self.alpha_ = alpha_
         self.sigma_ = sigma_
         self.lambda_ = lambda_
+        self.n_iter_ = iter_ + 1
         self._set_intercept(X_offset_, y_offset_, X_scale_)
         return self
 
