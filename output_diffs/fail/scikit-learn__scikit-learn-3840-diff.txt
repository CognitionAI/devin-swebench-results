diff --git a/sklearn/metrics/ranking.py b/sklearn/metrics/ranking.py
index 1d8d37954..17609ca0a 100644
--- a/sklearn/metrics/ranking.py
+++ b/sklearn/metrics/ranking.py
@@ -285,14 +285,33 @@ def roc_auc_score(y_true, y_score, average="macro", sample_weight=None):
     0.75
 
     """
-    def _binary_roc_auc_score(y_true, y_score, sample_weight=None):
+    def _binary_roc_auc_score(y_true, y_score, sample_weight=None, max_fpr=None):
+        """Binary roc auc score"""
         if len(np.unique(y_true)) != 2:
             raise ValueError("Only one class present in y_true. ROC AUC score "
                              "is not defined in that case.")
 
-        fpr, tpr, tresholds = roc_curve(y_true, y_score,
-                                        sample_weight=sample_weight)
-        return auc(fpr, tpr)
+        fpr, tpr, _ = roc_curve(y_true, y_score,
+                                 sample_weight=sample_weight)
+        if max_fpr is None or max_fpr == 1:
+            return auc(fpr, tpr)
+        if max_fpr <= 0 or max_fpr > 1:
+            raise ValueError("Expected max_fpr in range (0, 1], got: %r"
+                             % max_fpr)
+
+        # Find the index where we should stop the calculation
+        stop = np.searchsorted(fpr, max_fpr, 'right')
+        x_interp = [fpr[stop - 1], fpr[stop]]
+        y_interp = [tpr[stop - 1], tpr[stop]]
+        tpr_corrected = np.append(tpr[:stop], np.interp(max_fpr, x_interp, y_interp))
+        fpr_corrected = np.append(fpr[:stop], max_fpr)
+        partial_auc = auc(fpr_corrected, tpr_corrected)
+
+        # McClish correction: standardize result to be 0.5 if non-discriminant
+        # and 1 if perfect
+        min_area = 0.5 * max_fpr**2
+        max_area = max_fpr
+        return 0.5 * (1 + (partial_auc - min_area) / (max_area - min_area))
 
     y_type = type_of_target(y_true)
     if y_type == "binary":
@@ -549,16 +568,16 @@ def roc_curve(y_true, y_score, pos_label=None, sample_weight=None,
     Examples
     --------
     >>> import numpy as np
-    >>> from sklearn import metrics
-    >>> y = np.array([1, 1, 2, 2])
-    >>> scores = np.array([0.1, 0.4, 0.35, 0.8])
-    >>> fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=2)
+    >>> from sklearn.metrics import roc_curve
+    >>> y_true = np.array([0, 0, 1, 1])
+    >>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])
+    >>> fpr, tpr, thresholds = roc_curve(y_true, y_scores)
     >>> fpr
     array([ 0. ,  0. ,  0.5,  0.5,  1. ])
     >>> tpr
     array([ 0. ,  0.5,  0.5,  1. ,  1. ])
     >>> thresholds
-    array([ 1.8 ,  0.8 ,  0.4 ,  0.35,  0.1 ])
+    array([ 0.8 ,  0.8 ,  0.4 ,  0.35,  0.1 ])
 
     """
     fps, tps, thresholds = _binary_clf_curve(
